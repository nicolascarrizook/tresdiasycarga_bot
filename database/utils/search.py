"""
Database search utilities for Sistema Mayra.
"""
import logging
from typing import Dict, Any, List, Optional, Union
from datetime import datetime

from sqlalchemy import text, or_, and_, func
from sqlalchemy.ext.asyncio import AsyncSession

logger = logging.getLogger(__name__)


class SearchManager:
    """Database search manager with full-text search capabilities."""
    
    def __init__(self, db_connection=None):
        self.db_connection = db_connection
    
    async def setup_full_text_search(self) -> Dict[str, Any]:
        """Set up full-text search capabilities."""
        try:
            if not self.db_connection:
                from database.utils.connection import get_database_connection
                self.db_connection = get_database_connection()
            
            async with self.db_connection.get_async_session() as session:
                # Create text search configuration for Spanish
                await session.execute(text(\"\"\"\n                    DO $$ BEGIN\n                        CREATE TEXT SEARCH CONFIGURATION spanish_stem (\n                            COPY = spanish\n                        );\n                    EXCEPTION\n                        WHEN duplicate_object THEN null;\n                    END $$;\n                \"\"\"))\n                \n                # Create extensions if they don't exist\n                await session.execute(text(\"\"\"\n                    CREATE EXTENSION IF NOT EXISTS pg_trgm;\n                \"\"\"))\n                \n                await session.execute(text(\"\"\"\n                    CREATE EXTENSION IF NOT EXISTS unaccent;\n                \"\"\"))\n                \n                # Create function for normalized search\n                await session.execute(text(\"\"\"\n                    CREATE OR REPLACE FUNCTION normalize_search_text(text)\n                    RETURNS text AS $$\n                    SELECT lower(unaccent($1));\n                    $$ LANGUAGE SQL IMMUTABLE;\n                \"\"\"))\n                \n                await session.commit()\n                \n                logger.info(\"Full-text search setup completed\")\n                \n                return {\n                    \"success\": True,\n                    \"message\": \"Full-text search capabilities installed\",\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to setup full-text search: {e}\")\n            return {\"success\": False, \"error\": str(e)}\n    \n    async def create_search_indexes(self) -> Dict[str, Any]:\n        \"\"\"Create search indexes for better performance.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                # Create GIN indexes for full-text search\n                search_indexes = [\n                    {\n                        \"name\": \"idx_patients_search_gin\",\n                        \"sql\": \"CREATE INDEX IF NOT EXISTS idx_patients_search_gin ON patients USING gin (normalize_search_text(search_vector) gin_trgm_ops);\"\n                    },\n                    {\n                        \"name\": \"idx_recipes_search_gin\",\n                        \"sql\": \"CREATE INDEX IF NOT EXISTS idx_recipes_search_gin ON recipes USING gin (normalize_search_text(search_vector) gin_trgm_ops);\"\n                    },\n                    {\n                        \"name\": \"idx_recipes_name_gin\",\n                        \"sql\": \"CREATE INDEX IF NOT EXISTS idx_recipes_name_gin ON recipes USING gin (normalize_search_text(name) gin_trgm_ops);\"\n                    },\n                    {\n                        \"name\": \"idx_users_search_gin\",\n                        \"sql\": \"CREATE INDEX IF NOT EXISTS idx_users_search_gin ON users USING gin (normalize_search_text(full_name) gin_trgm_ops);\"\n                    },\n                    {\n                        \"name\": \"idx_embeddings_content_gin\",\n                        \"sql\": \"CREATE INDEX IF NOT EXISTS idx_embeddings_content_gin ON embeddings USING gin (normalize_search_text(content) gin_trgm_ops);\"\n                    }\n                ]\n                \n                created_indexes = []\n                \n                for index_info in search_indexes:\n                    try:\n                        await session.execute(text(index_info[\"sql\"]))\n                        created_indexes.append(index_info[\"name\"])\n                        logger.info(f\"Created search index: {index_info['name']}\")\n                    except Exception as e:\n                        logger.warning(f\"Failed to create index {index_info['name']}: {e}\")\n                \n                await session.commit()\n                \n                return {\n                    \"success\": True,\n                    \"created_indexes\": created_indexes,\n                    \"total_indexes\": len(search_indexes),\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to create search indexes: {e}\")\n            return {\"success\": False, \"error\": str(e)}\n    \n    async def search_recipes(self, query: str, filters: Optional[Dict[str, Any]] = None,\n                           limit: int = 20, offset: int = 0) -> Dict[str, Any]:\n        \"\"\"Search recipes with full-text search.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                # Build search query with ranking\n                search_query = text(\"\"\"\n                    SELECT \n                        r.id,\n                        r.name,\n                        r.description,\n                        r.category,\n                        r.subcategory,\n                        r.economic_level,\n                        r.difficulty,\n                        r.calories,\n                        r.protein,\n                        r.carbs,\n                        r.fat,\n                        r.cooking_time,\n                        r.servings,\n                        r.is_validated,\n                        r.usage_count,\n                        (\n                            CASE \n                                WHEN normalize_search_text(r.name) ILIKE '%' || normalize_search_text(:query) || '%' THEN 4\n                                WHEN normalize_search_text(r.description) ILIKE '%' || normalize_search_text(:query) || '%' THEN 3\n                                WHEN normalize_search_text(r.search_vector) ILIKE '%' || normalize_search_text(:query) || '%' THEN 2\n                                ELSE 1\n                            END\n                        ) as relevance_score,\n                        similarity(normalize_search_text(r.name), normalize_search_text(:query)) as name_similarity\n                    FROM recipes r\n                    WHERE r.is_active = true\n                    AND r.is_deleted = false\n                    AND (\n                        normalize_search_text(r.name) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR normalize_search_text(r.description) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR normalize_search_text(r.search_vector) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR similarity(normalize_search_text(r.name), normalize_search_text(:query)) > 0.3\n                    )\n                \"\"\")\n                \n                # Add filters\n                filter_conditions = []\n                params = {\"query\": query}\n                \n                if filters:\n                    if \"category\" in filters:\n                        filter_conditions.append(\"r.category = :category\")\n                        params[\"category\"] = filters[\"category\"]\n                    \n                    if \"economic_level\" in filters:\n                        filter_conditions.append(\"r.economic_level = :economic_level\")\n                        params[\"economic_level\"] = filters[\"economic_level\"]\n                    \n                    if \"difficulty\" in filters:\n                        filter_conditions.append(\"r.difficulty = :difficulty\")\n                        params[\"difficulty\"] = filters[\"difficulty\"]\n                    \n                    if \"max_calories\" in filters:\n                        filter_conditions.append(\"r.calories <= :max_calories\")\n                        params[\"max_calories\"] = filters[\"max_calories\"]\n                    \n                    if \"min_protein\" in filters:\n                        filter_conditions.append(\"r.protein >= :min_protein\")\n                        params[\"min_protein\"] = filters[\"min_protein\"]\n                    \n                    if \"max_cooking_time\" in filters:\n                        filter_conditions.append(\"(r.cooking_time <= :max_cooking_time OR r.cooking_time IS NULL)\")\n                        params[\"max_cooking_time\"] = filters[\"max_cooking_time\"]\n                    \n                    if \"validated_only\" in filters and filters[\"validated_only\"]:\n                        filter_conditions.append(\"r.is_validated = true\")\n                \n                # Add filter conditions to query\n                if filter_conditions:\n                    search_query = text(str(search_query) + \" AND \" + \" AND \".join(filter_conditions))\n                \n                # Add ordering and pagination\n                search_query = text(str(search_query) + \"\"\"\n                    ORDER BY relevance_score DESC, name_similarity DESC, r.usage_count DESC\n                    LIMIT :limit OFFSET :offset\n                \"\"\")\n                \n                params[\"limit\"] = limit\n                params[\"offset\"] = offset\n                \n                # Execute search\n                result = await session.execute(search_query, params)\n                \n                recipes = []\n                for row in result.fetchall():\n                    recipes.append({\n                        \"id\": row.id,\n                        \"name\": row.name,\n                        \"description\": row.description,\n                        \"category\": row.category,\n                        \"subcategory\": row.subcategory,\n                        \"economic_level\": row.economic_level,\n                        \"difficulty\": row.difficulty,\n                        \"calories\": row.calories,\n                        \"protein\": row.protein,\n                        \"carbs\": row.carbs,\n                        \"fat\": row.fat,\n                        \"cooking_time\": row.cooking_time,\n                        \"servings\": row.servings,\n                        \"is_validated\": row.is_validated,\n                        \"usage_count\": row.usage_count,\n                        \"relevance_score\": row.relevance_score,\n                        \"name_similarity\": round(row.name_similarity, 3)\n                    })\n                \n                # Count total results\n                count_query = text(\"\"\"\n                    SELECT COUNT(*)\n                    FROM recipes r\n                    WHERE r.is_active = true\n                    AND r.is_deleted = false\n                    AND (\n                        normalize_search_text(r.name) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR normalize_search_text(r.description) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR normalize_search_text(r.search_vector) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR similarity(normalize_search_text(r.name), normalize_search_text(:query)) > 0.3\n                    )\n                \"\"\")\n                \n                if filter_conditions:\n                    count_query = text(str(count_query) + \" AND \" + \" AND \".join(filter_conditions))\n                \n                count_result = await session.execute(count_query, params)\n                total_count = count_result.scalar()\n                \n                return {\n                    \"success\": True,\n                    \"query\": query,\n                    \"filters\": filters,\n                    \"results\": recipes,\n                    \"total_count\": total_count,\n                    \"page_size\": limit,\n                    \"offset\": offset,\n                    \"has_more\": (offset + limit) < total_count,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to search recipes: {e}\")\n            return {\"success\": False, \"error\": str(e)}\n    \n    async def search_patients(self, query: str, limit: int = 20, offset: int = 0) -> Dict[str, Any]:\n        \"\"\"Search patients with full-text search.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                search_query = text(\"\"\"\n                    SELECT \n                        p.id,\n                        p.name,\n                        p.email,\n                        p.telegram_username,\n                        p.telegram_first_name,\n                        p.telegram_last_name,\n                        p.age,\n                        p.sex,\n                        p.objective,\n                        p.activity_type,\n                        p.economic_level,\n                        p.is_active_patient,\n                        (\n                            CASE \n                                WHEN normalize_search_text(p.name) ILIKE '%' || normalize_search_text(:query) || '%' THEN 4\n                                WHEN normalize_search_text(p.email) ILIKE '%' || normalize_search_text(:query) || '%' THEN 3\n                                WHEN normalize_search_text(p.telegram_username) ILIKE '%' || normalize_search_text(:query) || '%' THEN 3\n                                WHEN normalize_search_text(p.search_vector) ILIKE '%' || normalize_search_text(:query) || '%' THEN 2\n                                ELSE 1\n                            END\n                        ) as relevance_score\n                    FROM patients p\n                    WHERE p.is_active = true\n                    AND p.is_deleted = false\n                    AND (\n                        normalize_search_text(p.name) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR normalize_search_text(p.email) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR normalize_search_text(p.telegram_username) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR normalize_search_text(p.search_vector) ILIKE '%' || normalize_search_text(:query) || '%'\n                    )\n                    ORDER BY relevance_score DESC, p.name ASC\n                    LIMIT :limit OFFSET :offset\n                \"\"\")\n                \n                result = await session.execute(search_query, {\n                    \"query\": query,\n                    \"limit\": limit,\n                    \"offset\": offset\n                })\n                \n                patients = []\n                for row in result.fetchall():\n                    patients.append({\n                        \"id\": row.id,\n                        \"name\": row.name,\n                        \"email\": row.email,\n                        \"telegram_username\": row.telegram_username,\n                        \"telegram_first_name\": row.telegram_first_name,\n                        \"telegram_last_name\": row.telegram_last_name,\n                        \"age\": row.age,\n                        \"sex\": row.sex,\n                        \"objective\": row.objective,\n                        \"activity_type\": row.activity_type,\n                        \"economic_level\": row.economic_level,\n                        \"is_active_patient\": row.is_active_patient,\n                        \"relevance_score\": row.relevance_score\n                    })\n                \n                # Count total results\n                count_query = text(\"\"\"\n                    SELECT COUNT(*)\n                    FROM patients p\n                    WHERE p.is_active = true\n                    AND p.is_deleted = false\n                    AND (\n                        normalize_search_text(p.name) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR normalize_search_text(p.email) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR normalize_search_text(p.telegram_username) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR normalize_search_text(p.search_vector) ILIKE '%' || normalize_search_text(:query) || '%'\n                    )\n                \"\"\")\n                \n                count_result = await session.execute(count_query, {\"query\": query})\n                total_count = count_result.scalar()\n                \n                return {\n                    \"success\": True,\n                    \"query\": query,\n                    \"results\": patients,\n                    \"total_count\": total_count,\n                    \"page_size\": limit,\n                    \"offset\": offset,\n                    \"has_more\": (offset + limit) < total_count,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to search patients: {e}\")\n            return {\"success\": False, \"error\": str(e)}\n    \n    async def search_embeddings(self, query: str, embedding_type: Optional[str] = None,\n                              limit: int = 20, offset: int = 0) -> Dict[str, Any]:\n        \"\"\"Search embeddings with full-text search.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                # Build query with optional type filter\n                base_query = \"\"\"\n                    SELECT \n                        e.id,\n                        e.content,\n                        e.embedding_type,\n                        e.source_type,\n                        e.source_id,\n                        e.language,\n                        e.content_length,\n                        e.word_count,\n                        e.quality_score,\n                        e.retrieval_count,\n                        similarity(normalize_search_text(e.content), normalize_search_text(:query)) as content_similarity\n                    FROM embeddings e\n                    WHERE e.is_active = true\n                    AND e.is_deleted = false\n                    AND (\n                        normalize_search_text(e.content) ILIKE '%' || normalize_search_text(:query) || '%'\n                        OR similarity(normalize_search_text(e.content), normalize_search_text(:query)) > 0.2\n                    )\n                \"\"\"\n                \n                params = {\"query\": query, \"limit\": limit, \"offset\": offset}\n                \n                if embedding_type:\n                    base_query += \" AND e.embedding_type = :embedding_type\"\n                    params[\"embedding_type\"] = embedding_type\n                \n                search_query = text(base_query + \"\"\"\n                    ORDER BY content_similarity DESC, e.quality_score DESC, e.retrieval_count DESC\n                    LIMIT :limit OFFSET :offset\n                \"\"\")\n                \n                result = await session.execute(search_query, params)\n                \n                embeddings = []\n                for row in result.fetchall():\n                    embeddings.append({\n                        \"id\": row.id,\n                        \"content\": row.content[:200] + \"...\" if len(row.content) > 200 else row.content,\n                        \"embedding_type\": row.embedding_type,\n                        \"source_type\": row.source_type,\n                        \"source_id\": row.source_id,\n                        \"language\": row.language,\n                        \"content_length\": row.content_length,\n                        \"word_count\": row.word_count,\n                        \"quality_score\": row.quality_score,\n                        \"retrieval_count\": row.retrieval_count,\n                        \"content_similarity\": round(row.content_similarity, 3)\n                    })\n                \n                # Count total results\n                count_query = text(base_query.replace(\"SELECT e.id,\", \"SELECT COUNT(*) FROM (\").replace(\"ORDER BY\", \"\").replace(\"LIMIT\", \"\").replace(\"OFFSET\", \"\") + \") as count_query\")\n                \n                count_result = await session.execute(count_query, params)\n                total_count = count_result.scalar()\n                \n                return {\n                    \"success\": True,\n                    \"query\": query,\n                    \"embedding_type\": embedding_type,\n                    \"results\": embeddings,\n                    \"total_count\": total_count,\n                    \"page_size\": limit,\n                    \"offset\": offset,\n                    \"has_more\": (offset + limit) < total_count,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to search embeddings: {e}\")\n            return {\"success\": False, \"error\": str(e)}\n    \n    async def global_search(self, query: str, limit: int = 20) -> Dict[str, Any]:\n        \"\"\"Search across all major entities.\"\"\"\n        try:\n            # Search in parallel across different entities\n            import asyncio\n            \n            search_tasks = [\n                self.search_recipes(query, limit=limit // 3),\n                self.search_patients(query, limit=limit // 3),\n                self.search_embeddings(query, limit=limit // 3)\n            ]\n            \n            results = await asyncio.gather(*search_tasks, return_exceptions=True)\n            \n            # Combine results\n            combined_results = {\n                \"success\": True,\n                \"query\": query,\n                \"recipes\": results[0] if not isinstance(results[0], Exception) else {\"error\": str(results[0])},\n                \"patients\": results[1] if not isinstance(results[1], Exception) else {\"error\": str(results[1])},\n                \"embeddings\": results[2] if not isinstance(results[2], Exception) else {\"error\": str(results[2])},\n                \"timestamp\": datetime.utcnow()\n            }\n            \n            return combined_results\n            \n        except Exception as e:\n            logger.error(f\"Failed to perform global search: {e}\")\n            return {\"success\": False, \"error\": str(e)}\n    \n    async def get_search_suggestions(self, query: str, limit: int = 5) -> Dict[str, Any]:\n        \"\"\"Get search suggestions based on partial query.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                # Get recipe name suggestions\n                recipe_suggestions_query = text(\"\"\"\n                    SELECT DISTINCT name\n                    FROM recipes\n                    WHERE normalize_search_text(name) ILIKE '%' || normalize_search_text(:query) || '%'\n                    AND is_active = true\n                    AND is_deleted = false\n                    ORDER BY name\n                    LIMIT :limit\n                \"\"\")\n                \n                recipe_result = await session.execute(recipe_suggestions_query, {\n                    \"query\": query,\n                    \"limit\": limit\n                })\n                \n                recipe_suggestions = [row.name for row in recipe_result.fetchall()]\n                \n                # Get patient name suggestions\n                patient_suggestions_query = text(\"\"\"\n                    SELECT DISTINCT name\n                    FROM patients\n                    WHERE normalize_search_text(name) ILIKE '%' || normalize_search_text(:query) || '%'\n                    AND is_active = true\n                    AND is_deleted = false\n                    ORDER BY name\n                    LIMIT :limit\n                \"\"\")\n                \n                patient_result = await session.execute(patient_suggestions_query, {\n                    \"query\": query,\n                    \"limit\": limit\n                })\n                \n                patient_suggestions = [row.name for row in patient_result.fetchall()]\n                \n                return {\n                    \"success\": True,\n                    \"query\": query,\n                    \"suggestions\": {\n                        \"recipes\": recipe_suggestions,\n                        \"patients\": patient_suggestions\n                    },\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to get search suggestions: {e}\")\n            return {\"success\": False, \"error\": str(e)}\n    \n    async def update_search_vectors(self, table_name: str, batch_size: int = 1000) -> Dict[str, Any]:\n        \"\"\"Update search vectors for a specific table.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                if table_name == \"recipes\":\n                    update_query = text(\"\"\"\n                        UPDATE recipes\n                        SET search_vector = CONCAT(\n                            COALESCE(name, ''), ' ',\n                            COALESCE(description, ''), ' ',\n                            COALESCE(category, ''), ' ',\n                            COALESCE(subcategory, ''), ' ',\n                            COALESCE(preparation, ''), ' ',\n                            COALESCE(array_to_string(dietary_restrictions, ' '), ''), ' ',\n                            COALESCE(array_to_string(allergens, ' '), ''), ' ',\n                            COALESCE(notes, '')\n                        )\n                        WHERE id IN (\n                            SELECT id FROM recipes\n                            WHERE is_active = true\n                            ORDER BY id\n                            LIMIT :batch_size\n                        )\n                    \"\"\")\n                elif table_name == \"patients\":\n                    update_query = text(\"\"\"\n                        UPDATE patients\n                        SET search_vector = CONCAT(\n                            COALESCE(name, ''), ' ',\n                            COALESCE(email, ''), ' ',\n                            COALESCE(telegram_username, ''), ' ',\n                            COALESCE(array_to_string(supplements, ' '), ''), ' ',\n                            COALESCE(array_to_string(pathologies, ' '), ''), ' ',\n                            COALESCE(array_to_string(restrictions, ' '), ''), ' ',\n                            COALESCE(array_to_string(preferences, ' '), ''), ' ',\n                            COALESCE(notes, '')\n                        )\n                        WHERE id IN (\n                            SELECT id FROM patients\n                            WHERE is_active = true\n                            ORDER BY id\n                            LIMIT :batch_size\n                        )\n                    \"\"\")\n                else:\n                    return {\"success\": False, \"error\": f\"Unsupported table: {table_name}\"}\n                \n                total_updated = 0\n                \n                while True:\n                    result = await session.execute(update_query, {\"batch_size\": batch_size})\n                    updated_count = result.rowcount\n                    \n                    if updated_count == 0:\n                        break\n                    \n                    total_updated += updated_count\n                    await session.commit()\n                    \n                    logger.info(f\"Updated {updated_count} search vectors in {table_name}\")\n                    \n                    if updated_count < batch_size:\n                        break\n                \n                return {\n                    \"success\": True,\n                    \"table_name\": table_name,\n                    \"updated_count\": total_updated,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to update search vectors for {table_name}: {e}\")\n            return {\"success\": False, \"error\": str(e)}\n    \n    async def get_search_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get search performance statistics.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                # Check if search indexes exist\n                index_check_query = text(\"\"\"\n                    SELECT indexname\n                    FROM pg_indexes\n                    WHERE schemaname = 'public'\n                    AND indexname LIKE '%search%'\n                \"\"\")\n                \n                index_result = await session.execute(index_check_query)\n                search_indexes = [row.indexname for row in index_result.fetchall()]\n                \n                # Check search vector coverage\n                coverage_queries = {\n                    \"recipes\": text(\"SELECT COUNT(*) as total, COUNT(search_vector) as with_vector FROM recipes WHERE is_active = true\"),\n                    \"patients\": text(\"SELECT COUNT(*) as total, COUNT(search_vector) as with_vector FROM patients WHERE is_active = true\")\n                }\n                \n                coverage_stats = {}\n                for table_name, query in coverage_queries.items():\n                    result = await session.execute(query)\n                    row = result.fetchone()\n                    coverage_stats[table_name] = {\n                        \"total_records\": row.total,\n                        \"records_with_search_vector\": row.with_vector,\n                        \"coverage_percentage\": round((row.with_vector / row.total) * 100, 2) if row.total > 0 else 0\n                    }\n                \n                return {\n                    \"success\": True,\n                    \"search_indexes\": search_indexes,\n                    \"search_vector_coverage\": coverage_stats,\n                    \"extensions_status\": {\n                        \"pg_trgm\": True,  # Assume installed if no error\n                        \"unaccent\": True\n                    },\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to get search statistics: {e}\")\n            return {\"success\": False, \"error\": str(e)}"