"""
Database indexing utilities for Sistema Mayra.
"""
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime

from sqlalchemy import text, inspect
from sqlalchemy.ext.asyncio import AsyncSession

logger = logging.getLogger(__name__)


class IndexManager:
    """Database index management utilities."""
    
    def __init__(self, db_connection=None):
        self.db_connection = db_connection
    
    async def analyze_index_usage(self) -> Dict[str, Any]:
        """Analyze index usage statistics."""
        try:
            if not self.db_connection:
                from database.utils.connection import get_database_connection
                self.db_connection = get_database_connection()
            
            async with self.db_connection.get_async_session() as session:
                # Get index usage statistics
                usage_query = text(\"\"\"\n                    SELECT \n                        schemaname,\n                        tablename,\n                        indexname,\n                        idx_tup_read,\n                        idx_tup_fetch,\n                        idx_scan,\n                        idx_blks_read,\n                        idx_blks_hit,\n                        CASE \n                            WHEN idx_scan = 0 THEN 'unused'\n                            WHEN idx_scan < 100 THEN 'low_usage'\n                            WHEN idx_scan < 1000 THEN 'medium_usage'\n                            ELSE 'high_usage'\n                        END as usage_category\n                    FROM pg_stat_user_indexes\n                    ORDER BY idx_scan DESC\n                \"\"\")\n                \n                result = await session.execute(usage_query)\n                indexes = []\n                \n                for row in result.fetchall():\n                    indexes.append({\n                        \"schema\": row.schemaname,\n                        \"table\": row.tablename,\n                        \"index\": row.indexname,\n                        \"tuples_read\": row.idx_tup_read,\n                        \"tuples_fetched\": row.idx_tup_fetch,\n                        \"scans\": row.idx_scan,\n                        \"blocks_read\": row.idx_blks_read,\n                        \"blocks_hit\": row.idx_blks_hit,\n                        \"usage_category\": row.usage_category,\n                        \"hit_ratio\": round(\n                            (row.idx_blks_hit / (row.idx_blks_hit + row.idx_blks_read)) * 100, 2\n                        ) if (row.idx_blks_hit + row.idx_blks_read) > 0 else 0\n                    })\n                \n                # Categorize indexes\n                unused_indexes = [idx for idx in indexes if idx[\"usage_category\"] == \"unused\"]\n                low_usage_indexes = [idx for idx in indexes if idx[\"usage_category\"] == \"low_usage\"]\n                \n                return {\n                    \"total_indexes\": len(indexes),\n                    \"unused_indexes\": len(unused_indexes),\n                    \"low_usage_indexes\": len(low_usage_indexes),\n                    \"indexes\": indexes,\n                    \"unused_index_details\": unused_indexes,\n                    \"low_usage_index_details\": low_usage_indexes,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to analyze index usage: {e}\")\n            return {\"error\": str(e)}\n    \n    async def get_index_sizes(self) -> Dict[str, Any]:\n        \"\"\"Get index sizes.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                size_query = text(\"\"\"\n                    SELECT \n                        schemaname,\n                        tablename,\n                        indexname,\n                        pg_size_pretty(pg_relation_size(indexrelid)) as size_pretty,\n                        pg_relation_size(indexrelid) as size_bytes\n                    FROM pg_stat_user_indexes\n                    ORDER BY pg_relation_size(indexrelid) DESC\n                \"\"\")\n                \n                result = await session.execute(size_query)\n                indexes = []\n                total_size = 0\n                \n                for row in result.fetchall():\n                    size_bytes = row.size_bytes\n                    total_size += size_bytes\n                    \n                    indexes.append({\n                        \"schema\": row.schemaname,\n                        \"table\": row.tablename,\n                        \"index\": row.indexname,\n                        \"size_pretty\": row.size_pretty,\n                        \"size_bytes\": size_bytes\n                    })\n                \n                return {\n                    \"total_indexes\": len(indexes),\n                    \"total_size_bytes\": total_size,\n                    \"total_size_pretty\": self._format_size(total_size),\n                    \"indexes\": indexes,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to get index sizes: {e}\")\n            return {\"error\": str(e)}\n    \n    async def find_duplicate_indexes(self) -> Dict[str, Any]:\n        \"\"\"Find duplicate indexes.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                # Find indexes with same column definitions\n                duplicate_query = text(\"\"\"\n                    SELECT \n                        schemaname,\n                        tablename,\n                        array_agg(indexname) as index_names,\n                        array_agg(indexdef) as index_definitions,\n                        count(*) as duplicate_count\n                    FROM pg_indexes\n                    WHERE schemaname = 'public'\n                    GROUP BY schemaname, tablename, indexdef\n                    HAVING count(*) > 1\n                    ORDER BY duplicate_count DESC\n                \"\"\")\n                \n                result = await session.execute(duplicate_query)\n                duplicates = []\n                \n                for row in result.fetchall():\n                    duplicates.append({\n                        \"schema\": row.schemaname,\n                        \"table\": row.tablename,\n                        \"index_names\": row.index_names,\n                        \"index_definitions\": row.index_definitions,\n                        \"duplicate_count\": row.duplicate_count\n                    })\n                \n                return {\n                    \"duplicate_groups\": len(duplicates),\n                    \"total_duplicate_indexes\": sum(d[\"duplicate_count\"] for d in duplicates),\n                    \"duplicates\": duplicates,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to find duplicate indexes: {e}\")\n            return {\"error\": str(e)}\n    \n    async def get_missing_indexes(self) -> Dict[str, Any]:\n        \"\"\"Identify potentially missing indexes based on query patterns.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                # Get tables without indexes on foreign key columns\n                missing_fk_indexes_query = text(\"\"\"\n                    SELECT \n                        t.table_name,\n                        t.column_name,\n                        t.constraint_name\n                    FROM information_schema.table_constraints tc\n                    JOIN information_schema.key_column_usage t \n                        ON tc.constraint_name = t.constraint_name\n                    WHERE tc.constraint_type = 'FOREIGN KEY'\n                    AND t.table_schema = 'public'\n                    AND NOT EXISTS (\n                        SELECT 1 FROM pg_indexes i\n                        WHERE i.tablename = t.table_name\n                        AND i.indexdef LIKE '%' || t.column_name || '%'\n                    )\n                \"\"\")\n                \n                result = await session.execute(missing_fk_indexes_query)\n                missing_fk_indexes = []\n                \n                for row in result.fetchall():\n                    missing_fk_indexes.append({\n                        \"table\": row.table_name,\n                        \"column\": row.column_name,\n                        \"constraint\": row.constraint_name,\n                        \"index_type\": \"foreign_key\",\n                        \"suggested_index\": f\"CREATE INDEX idx_{row.table_name}_{row.column_name} ON {row.table_name} ({row.column_name});\"\n                    })\n                \n                # Get frequently queried columns without indexes\n                # This is a simplified heuristic - in practice, you'd analyze query logs\n                common_query_columns = [\n                    (\"users\", \"email\"),\n                    (\"users\", \"username\"),\n                    (\"patients\", \"telegram_user_id\"),\n                    (\"conversations\", \"telegram_chat_id\"),\n                    (\"plans\", \"patient_id\"),\n                    (\"recipes\", \"category\"),\n                    (\"embeddings\", \"embedding_type\")\n                ]\n                \n                missing_query_indexes = []\n                for table_name, column_name in common_query_columns:\n                    index_exists_query = text(\"\"\"\n                        SELECT 1 FROM pg_indexes \n                        WHERE tablename = :table_name\n                        AND indexdef LIKE '%' || :column_name || '%'\n                    \"\"\")\n                    \n                    result = await session.execute(index_exists_query, {\n                        \"table_name\": table_name,\n                        \"column_name\": column_name\n                    })\n                    \n                    if not result.fetchone():\n                        missing_query_indexes.append({\n                            \"table\": table_name,\n                            \"column\": column_name,\n                            \"index_type\": \"query_optimization\",\n                            \"suggested_index\": f\"CREATE INDEX idx_{table_name}_{column_name} ON {table_name} ({column_name});\"\n                        })\n                \n                return {\n                    \"missing_foreign_key_indexes\": len(missing_fk_indexes),\n                    \"missing_query_indexes\": len(missing_query_indexes),\n                    \"total_missing\": len(missing_fk_indexes) + len(missing_query_indexes),\n                    \"foreign_key_indexes\": missing_fk_indexes,\n                    \"query_indexes\": missing_query_indexes,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to find missing indexes: {e}\")\n            return {\"error\": str(e)}\n    \n    async def create_index(self, table_name: str, column_names: List[str], \n                          index_name: Optional[str] = None,\n                          index_type: str = \"btree\",\n                          unique: bool = False,\n                          concurrent: bool = True) -> Dict[str, Any]:\n        \"\"\"Create database index.\"\"\"\n        try:\n            if index_name is None:\n                index_name = f\"idx_{table_name}_{'_'.join(column_names)}\"\n            \n            columns_str = \", \".join(column_names)\n            \n            # Build CREATE INDEX statement\n            create_parts = [\"CREATE\"]\n            \n            if unique:\n                create_parts.append(\"UNIQUE\")\n            \n            create_parts.append(\"INDEX\")\n            \n            if concurrent:\n                create_parts.append(\"CONCURRENTLY\")\n            \n            create_parts.extend([\n                index_name,\n                \"ON\",\n                table_name,\n                f\"USING {index_type}\",\n                f\"({columns_str})\"\n            ])\n            \n            create_sql = \" \".join(create_parts)\n            \n            async with self.db_connection.get_async_session() as session:\n                start_time = datetime.utcnow()\n                \n                await session.execute(text(create_sql))\n                await session.commit()\n                \n                execution_time = (datetime.utcnow() - start_time).total_seconds()\n                \n                logger.info(f\"Created index {index_name} in {execution_time:.2f}s\")\n                \n                return {\n                    \"success\": True,\n                    \"index_name\": index_name,\n                    \"table_name\": table_name,\n                    \"columns\": column_names,\n                    \"index_type\": index_type,\n                    \"unique\": unique,\n                    \"concurrent\": concurrent,\n                    \"execution_time\": execution_time,\n                    \"sql\": create_sql,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to create index {index_name}: {e}\")\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"index_name\": index_name,\n                \"table_name\": table_name,\n                \"columns\": column_names\n            }\n    \n    async def drop_index(self, index_name: str, concurrent: bool = True) -> Dict[str, Any]:\n        \"\"\"Drop database index.\"\"\"\n        try:\n            drop_parts = [\"DROP\", \"INDEX\"]\n            \n            if concurrent:\n                drop_parts.append(\"CONCURRENTLY\")\n            \n            drop_parts.append(index_name)\n            \n            drop_sql = \" \".join(drop_parts)\n            \n            async with self.db_connection.get_async_session() as session:\n                start_time = datetime.utcnow()\n                \n                await session.execute(text(drop_sql))\n                await session.commit()\n                \n                execution_time = (datetime.utcnow() - start_time).total_seconds()\n                \n                logger.info(f\"Dropped index {index_name} in {execution_time:.2f}s\")\n                \n                return {\n                    \"success\": True,\n                    \"index_name\": index_name,\n                    \"concurrent\": concurrent,\n                    \"execution_time\": execution_time,\n                    \"sql\": drop_sql,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to drop index {index_name}: {e}\")\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"index_name\": index_name\n            }\n    \n    async def reindex_table(self, table_name: str, concurrent: bool = True) -> Dict[str, Any]:\n        \"\"\"Reindex table.\"\"\"\n        try:\n            reindex_parts = [\"REINDEX\"]\n            \n            if concurrent:\n                reindex_parts.append(\"CONCURRENTLY\")\n            \n            reindex_parts.extend([\"TABLE\", table_name])\n            \n            reindex_sql = \" \".join(reindex_parts)\n            \n            async with self.db_connection.get_async_session() as session:\n                start_time = datetime.utcnow()\n                \n                await session.execute(text(reindex_sql))\n                await session.commit()\n                \n                execution_time = (datetime.utcnow() - start_time).total_seconds()\n                \n                logger.info(f\"Reindexed table {table_name} in {execution_time:.2f}s\")\n                \n                return {\n                    \"success\": True,\n                    \"table_name\": table_name,\n                    \"concurrent\": concurrent,\n                    \"execution_time\": execution_time,\n                    \"sql\": reindex_sql,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to reindex table {table_name}: {e}\")\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"table_name\": table_name\n            }\n    \n    async def get_index_recommendations(self) -> Dict[str, Any]:\n        \"\"\"Get index recommendations based on analysis.\"\"\"\n        try:\n            recommendations = []\n            \n            # Analyze index usage\n            usage_analysis = await self.analyze_index_usage()\n            if \"error\" not in usage_analysis:\n                # Recommend dropping unused indexes\n                for unused_idx in usage_analysis.get(\"unused_index_details\", []):\n                    recommendations.append({\n                        \"type\": \"drop_unused\",\n                        \"priority\": \"low\",\n                        \"table\": unused_idx[\"table\"],\n                        \"index\": unused_idx[\"index\"],\n                        \"reason\": \"Index is not being used\",\n                        \"action\": f\"DROP INDEX {unused_idx['index']}\",\n                        \"estimated_benefit\": \"Reduced storage and maintenance overhead\"\n                    })\n            \n            # Find missing indexes\n            missing_analysis = await self.get_missing_indexes()\n            if \"error\" not in missing_analysis:\n                # Recommend creating missing foreign key indexes\n                for missing_idx in missing_analysis.get(\"foreign_key_indexes\", []):\n                    recommendations.append({\n                        \"type\": \"create_foreign_key\",\n                        \"priority\": \"high\",\n                        \"table\": missing_idx[\"table\"],\n                        \"column\": missing_idx[\"column\"],\n                        \"reason\": \"Foreign key column without index\",\n                        \"action\": missing_idx[\"suggested_index\"],\n                        \"estimated_benefit\": \"Improved JOIN performance\"\n                    })\n                \n                # Recommend creating query optimization indexes\n                for missing_idx in missing_analysis.get(\"query_indexes\", []):\n                    recommendations.append({\n                        \"type\": \"create_query_optimization\",\n                        \"priority\": \"medium\",\n                        \"table\": missing_idx[\"table\"],\n                        \"column\": missing_idx[\"column\"],\n                        \"reason\": \"Frequently queried column without index\",\n                        \"action\": missing_idx[\"suggested_index\"],\n                        \"estimated_benefit\": \"Improved query performance\"\n                    })\n            \n            # Find duplicate indexes\n            duplicate_analysis = await self.find_duplicate_indexes()\n            if \"error\" not in duplicate_analysis:\n                for duplicate_group in duplicate_analysis.get(\"duplicates\", []):\n                    # Recommend keeping one and dropping others\n                    indexes_to_drop = duplicate_group[\"index_names\"][1:]  # Keep first, drop rest\n                    for idx_name in indexes_to_drop:\n                        recommendations.append({\n                            \"type\": \"drop_duplicate\",\n                            \"priority\": \"medium\",\n                            \"table\": duplicate_group[\"table\"],\n                            \"index\": idx_name,\n                            \"reason\": \"Duplicate index\",\n                            \"action\": f\"DROP INDEX {idx_name}\",\n                            \"estimated_benefit\": \"Reduced storage and maintenance overhead\"\n                        })\n            \n            # Sort recommendations by priority\n            priority_order = {\"high\": 0, \"medium\": 1, \"low\": 2}\n            recommendations.sort(key=lambda x: priority_order.get(x[\"priority\"], 3))\n            \n            return {\n                \"total_recommendations\": len(recommendations),\n                \"high_priority\": len([r for r in recommendations if r[\"priority\"] == \"high\"]),\n                \"medium_priority\": len([r for r in recommendations if r[\"priority\"] == \"medium\"]),\n                \"low_priority\": len([r for r in recommendations if r[\"priority\"] == \"low\"]),\n                \"recommendations\": recommendations,\n                \"timestamp\": datetime.utcnow()\n            }\n            \n        except Exception as e:\n            logger.error(f\"Failed to get index recommendations: {e}\")\n            return {\"error\": str(e)}\n    \n    def _format_size(self, size_bytes: int) -> str:\n        \"\"\"Format size in human readable format.\"\"\"\n        for unit in ['B', 'KB', 'MB', 'GB']:\n            if size_bytes < 1024.0:\n                return f\"{size_bytes:.1f} {unit}\"\n            size_bytes /= 1024.0\n        return f\"{size_bytes:.1f} TB\"\n    \n    async def get_index_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive index statistics.\"\"\"\n        try:\n            usage_stats = await self.analyze_index_usage()\n            size_stats = await self.get_index_sizes()\n            duplicate_stats = await self.find_duplicate_indexes()\n            missing_stats = await self.get_missing_indexes()\n            \n            return {\n                \"usage_analysis\": usage_stats,\n                \"size_analysis\": size_stats,\n                \"duplicate_analysis\": duplicate_stats,\n                \"missing_analysis\": missing_stats,\n                \"timestamp\": datetime.utcnow()\n            }\n            \n        except Exception as e:\n            logger.error(f\"Failed to get index statistics: {e}\")\n            return {\"error\": str(e)}"