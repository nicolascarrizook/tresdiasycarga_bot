"""
Database performance monitoring utilities for Sistema Mayra.
"""
import logging
import time
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass
from functools import wraps

from sqlalchemy import text, func
from sqlalchemy.ext.asyncio import AsyncSession

logger = logging.getLogger(__name__)


@dataclass
class QueryMetrics:
    """Query performance metrics container."""
    query_hash: str
    query_text: str
    execution_time: float
    rows_returned: int
    timestamp: datetime
    database_time: float
    user_id: Optional[int] = None
    session_id: Optional[str] = None


class PerformanceMonitor:
    """Database performance monitoring and profiling."""
    
    def __init__(self, db_connection=None):
        self.db_connection = db_connection
        self.query_metrics: List[QueryMetrics] = []
        self.slow_query_threshold = 1.0  # 1 second
        self.max_metrics_history = 10000
    
    def query_profiler(self, user_id: Optional[int] = None, session_id: Optional[str] = None):
        """Decorator to profile database queries."""
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                start_time = time.time()
                
                try:
                    result = await func(*args, **kwargs)
                    
                    execution_time = time.time() - start_time
                    
                    # Create query metrics
                    query_hash = hash(str(func.__name__) + str(args) + str(kwargs))
                    
                    metrics = QueryMetrics(
                        query_hash=str(query_hash),
                        query_text=f"{func.__name__}({args}, {kwargs})",
                        execution_time=execution_time,
                        rows_returned=len(result) if isinstance(result, list) else 1,
                        timestamp=datetime.utcnow(),
                        database_time=execution_time,  # Simplified
                        user_id=user_id,
                        session_id=session_id
                    )
                    
                    self._record_metrics(metrics)
                    
                    # Log slow queries
                    if execution_time > self.slow_query_threshold:
                        logger.warning(f"Slow query detected: {func.__name__} took {execution_time:.2f}s")
                    
                    return result
                    
                except Exception as e:
                    execution_time = time.time() - start_time
                    logger.error(f"Query failed: {func.__name__} after {execution_time:.2f}s - {e}")
                    raise
            
            return wrapper
        return decorator
    
    def _record_metrics(self, metrics: QueryMetrics):
        """Record query metrics."""
        self.query_metrics.append(metrics)
        
        # Keep only recent metrics
        if len(self.query_metrics) > self.max_metrics_history:
            self.query_metrics = self.query_metrics[-self.max_metrics_history:]
    
    async def get_database_statistics(self) -> Dict[str, Any]:
        """Get comprehensive database statistics."""
        try:
            if not self.db_connection:
                from database.utils.connection import get_database_connection
                self.db_connection = get_database_connection()
            
            async with self.db_connection.get_async_session() as session:
                # Database size and activity
                db_stats_query = text(\"\"\"\n                    SELECT \n                        pg_database_size(current_database()) as db_size,\n                        (SELECT count(*) FROM pg_stat_activity WHERE datname = current_database()) as active_connections,\n                        (SELECT count(*) FROM pg_stat_activity WHERE datname = current_database() AND state = 'active') as active_queries,\n                        (SELECT count(*) FROM pg_stat_activity WHERE datname = current_database() AND state = 'idle in transaction') as idle_in_transaction\n                \"\"\")\n                \n                db_result = await session.execute(db_stats_query)\n                db_stats = db_result.fetchone()\n                \n                # Table statistics\n                table_stats_query = text(\"\"\"\n                    SELECT \n                        schemaname,\n                        tablename,\n                        n_tup_ins as inserts,\n                        n_tup_upd as updates,\n                        n_tup_del as deletes,\n                        n_live_tup as live_tuples,\n                        n_dead_tup as dead_tuples,\n                        seq_scan as sequential_scans,\n                        seq_tup_read as sequential_reads,\n                        idx_scan as index_scans,\n                        idx_tup_fetch as index_reads,\n                        n_tup_hot_upd as hot_updates,\n                        n_tup_newpage_upd as newpage_updates\n                    FROM pg_stat_user_tables\n                    ORDER BY n_live_tup DESC\n                \"\"\")\n                \n                table_result = await session.execute(table_stats_query)\n                table_stats = []\n                \n                for row in table_result.fetchall():\n                    table_stats.append({\n                        \"schema\": row.schemaname,\n                        \"table\": row.tablename,\n                        \"inserts\": row.inserts,\n                        \"updates\": row.updates,\n                        \"deletes\": row.deletes,\n                        \"live_tuples\": row.live_tuples,\n                        \"dead_tuples\": row.dead_tuples,\n                        \"sequential_scans\": row.sequential_scans,\n                        \"sequential_reads\": row.sequential_reads,\n                        \"index_scans\": row.index_scans,\n                        \"index_reads\": row.index_reads,\n                        \"hot_updates\": row.hot_updates,\n                        \"newpage_updates\": row.newpage_updates,\n                        \"dead_tuple_ratio\": round(row.dead_tuples / (row.live_tuples + 1) * 100, 2)\n                    })\n                \n                # Index usage statistics\n                index_stats_query = text(\"\"\"\n                    SELECT \n                        schemaname,\n                        tablename,\n                        indexname,\n                        idx_scan as scans,\n                        idx_tup_read as tuples_read,\n                        idx_tup_fetch as tuples_fetched,\n                        idx_blks_read as blocks_read,\n                        idx_blks_hit as blocks_hit\n                    FROM pg_stat_user_indexes\n                    WHERE idx_scan > 0\n                    ORDER BY idx_scan DESC\n                    LIMIT 20\n                \"\"\")\n                \n                index_result = await session.execute(index_stats_query)\n                index_stats = []\n                \n                for row in index_result.fetchall():\n                    hit_ratio = round(\n                        (row.blocks_hit / (row.blocks_hit + row.blocks_read)) * 100, 2\n                    ) if (row.blocks_hit + row.blocks_read) > 0 else 0\n                    \n                    index_stats.append({\n                        \"schema\": row.schemaname,\n                        \"table\": row.tablename,\n                        \"index\": row.indexname,\n                        \"scans\": row.scans,\n                        \"tuples_read\": row.tuples_read,\n                        \"tuples_fetched\": row.tuples_fetched,\n                        \"blocks_read\": row.blocks_read,\n                        \"blocks_hit\": row.blocks_hit,\n                        \"hit_ratio\": hit_ratio\n                    })\n                \n                # Cache hit ratios\n                cache_stats_query = text(\"\"\"\n                    SELECT \n                        sum(heap_blks_read) as heap_read,\n                        sum(heap_blks_hit) as heap_hit,\n                        sum(idx_blks_read) as idx_read,\n                        sum(idx_blks_hit) as idx_hit\n                    FROM pg_statio_user_tables\n                \"\"\")\n                \n                cache_result = await session.execute(cache_stats_query)\n                cache_stats = cache_result.fetchone()\n                \n                heap_hit_ratio = round(\n                    (cache_stats.heap_hit / (cache_stats.heap_hit + cache_stats.heap_read)) * 100, 2\n                ) if (cache_stats.heap_hit + cache_stats.heap_read) > 0 else 0\n                \n                index_hit_ratio = round(\n                    (cache_stats.idx_hit / (cache_stats.idx_hit + cache_stats.idx_read)) * 100, 2\n                ) if (cache_stats.idx_hit + cache_stats.idx_read) > 0 else 0\n                \n                return {\n                    \"database\": {\n                        \"size_bytes\": db_stats.db_size,\n                        \"size_pretty\": self._format_size(db_stats.db_size),\n                        \"active_connections\": db_stats.active_connections,\n                        \"active_queries\": db_stats.active_queries,\n                        \"idle_in_transaction\": db_stats.idle_in_transaction\n                    },\n                    \"cache\": {\n                        \"heap_hit_ratio\": heap_hit_ratio,\n                        \"index_hit_ratio\": index_hit_ratio,\n                        \"overall_hit_ratio\": round((heap_hit_ratio + index_hit_ratio) / 2, 2)\n                    },\n                    \"tables\": table_stats,\n                    \"top_indexes\": index_stats,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to get database statistics: {e}\")\n            return {\"error\": str(e)}\n    \n    async def get_slow_queries(self, hours: int = 24) -> Dict[str, Any]:\n        \"\"\"Get slow queries from pg_stat_statements if available.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                # Check if pg_stat_statements is available\n                extension_check = text(\"\"\"\n                    SELECT EXISTS (\n                        SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements'\n                    )\n                \"\"\")\n                \n                extension_result = await session.execute(extension_check)\n                has_pg_stat_statements = extension_result.scalar()\n                \n                if has_pg_stat_statements:\n                    # Get slow queries from pg_stat_statements\n                    slow_queries_query = text(\"\"\"\n                        SELECT \n                            query,\n                            calls,\n                            total_exec_time,\n                            mean_exec_time,\n                            stddev_exec_time,\n                            rows,\n                            100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\n                        FROM pg_stat_statements\n                        WHERE mean_exec_time > :threshold\n                        ORDER BY mean_exec_time DESC\n                        LIMIT 20\n                    \"\"\")\n                    \n                    slow_result = await session.execute(slow_queries_query, {\n                        \"threshold\": self.slow_query_threshold * 1000  # Convert to milliseconds\n                    })\n                    \n                    slow_queries = []\n                    for row in slow_result.fetchall():\n                        slow_queries.append({\n                            \"query\": row.query[:200] + \"...\" if len(row.query) > 200 else row.query,\n                            \"calls\": row.calls,\n                            \"total_exec_time_ms\": round(row.total_exec_time, 2),\n                            \"mean_exec_time_ms\": round(row.mean_exec_time, 2),\n                            \"stddev_exec_time_ms\": round(row.stddev_exec_time, 2),\n                            \"rows\": row.rows,\n                            \"hit_percent\": round(row.hit_percent, 2) if row.hit_percent else 0\n                        })\n                    \n                    return {\n                        \"success\": True,\n                        \"source\": \"pg_stat_statements\",\n                        \"slow_queries\": slow_queries,\n                        \"threshold_ms\": self.slow_query_threshold * 1000,\n                        \"timestamp\": datetime.utcnow()\n                    }\n                else:\n                    # Fall back to application-level metrics\n                    cutoff_time = datetime.utcnow() - timedelta(hours=hours)\n                    slow_queries = [\n                        {\n                            \"query\": metric.query_text,\n                            \"execution_time_s\": metric.execution_time,\n                            \"rows_returned\": metric.rows_returned,\n                            \"timestamp\": metric.timestamp,\n                            \"user_id\": metric.user_id,\n                            \"session_id\": metric.session_id\n                        }\n                        for metric in self.query_metrics\n                        if metric.execution_time > self.slow_query_threshold\n                        and metric.timestamp >= cutoff_time\n                    ]\n                    \n                    # Sort by execution time\n                    slow_queries.sort(key=lambda x: x[\"execution_time_s\"], reverse=True)\n                    \n                    return {\n                        \"success\": True,\n                        \"source\": \"application_metrics\",\n                        \"slow_queries\": slow_queries[:20],  # Top 20\n                        \"threshold_s\": self.slow_query_threshold,\n                        \"timestamp\": datetime.utcnow()\n                    }\n                \n        except Exception as e:\n            logger.error(f\"Failed to get slow queries: {e}\")\n            return {\"success\": False, \"error\": str(e)}\n    \n    async def get_query_performance_summary(self, hours: int = 24) -> Dict[str, Any]:\n        \"\"\"Get query performance summary from application metrics.\"\"\"\n        try:\n            cutoff_time = datetime.utcnow() - timedelta(hours=hours)\n            \n            # Filter recent metrics\n            recent_metrics = [\n                metric for metric in self.query_metrics\n                if metric.timestamp >= cutoff_time\n            ]\n            \n            if not recent_metrics:\n                return {\n                    \"success\": True,\n                    \"total_queries\": 0,\n                    \"message\": \"No query metrics available\",\n                    \"timestamp\": datetime.utcnow()\n                }\n            \n            # Calculate statistics\n            total_queries = len(recent_metrics)\n            total_execution_time = sum(metric.execution_time for metric in recent_metrics)\n            avg_execution_time = total_execution_time / total_queries\n            \n            # Find slowest queries\n            slowest_queries = sorted(\n                recent_metrics, \n                key=lambda x: x.execution_time, \n                reverse=True\n            )[:10]\n            \n            # Count slow queries\n            slow_queries_count = sum(\n                1 for metric in recent_metrics\n                if metric.execution_time > self.slow_query_threshold\n            )\n            \n            # Group by query hash for frequency analysis\n            query_frequency = {}\n            for metric in recent_metrics:\n                if metric.query_hash in query_frequency:\n                    query_frequency[metric.query_hash][\"count\"] += 1\n                    query_frequency[metric.query_hash][\"total_time\"] += metric.execution_time\n                else:\n                    query_frequency[metric.query_hash] = {\n                        \"query\": metric.query_text,\n                        \"count\": 1,\n                        \"total_time\": metric.execution_time\n                    }\n            \n            # Calculate average time per query type\n            for query_hash, stats in query_frequency.items():\n                stats[\"avg_time\"] = stats[\"total_time\"] / stats[\"count\"]\n            \n            # Sort by frequency\n            most_frequent = sorted(\n                query_frequency.values(),\n                key=lambda x: x[\"count\"],\n                reverse=True\n            )[:10]\n            \n            return {\n                \"success\": True,\n                \"total_queries\": total_queries,\n                \"total_execution_time\": round(total_execution_time, 2),\n                \"average_execution_time\": round(avg_execution_time, 3),\n                \"slow_queries_count\": slow_queries_count,\n                \"slow_queries_percentage\": round((slow_queries_count / total_queries) * 100, 2),\n                \"slowest_queries\": [\n                    {\n                        \"query\": metric.query_text,\n                        \"execution_time\": round(metric.execution_time, 3),\n                        \"rows_returned\": metric.rows_returned,\n                        \"timestamp\": metric.timestamp\n                    }\n                    for metric in slowest_queries\n                ],\n                \"most_frequent_queries\": [\n                    {\n                        \"query\": stats[\"query\"],\n                        \"count\": stats[\"count\"],\n                        \"total_time\": round(stats[\"total_time\"], 2),\n                        \"avg_time\": round(stats[\"avg_time\"], 3)\n                    }\n                    for stats in most_frequent\n                ],\n                \"time_period_hours\": hours,\n                \"timestamp\": datetime.utcnow()\n            }\n            \n        except Exception as e:\n            logger.error(f\"Failed to get query performance summary: {e}\")\n            return {\"success\": False, \"error\": str(e)}\n    \n    async def analyze_table_performance(self, table_name: str) -> Dict[str, Any]:\n        \"\"\"Analyze performance for a specific table.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                # Table statistics\n                table_stats_query = text(\"\"\"\n                    SELECT \n                        schemaname,\n                        tablename,\n                        n_tup_ins as inserts,\n                        n_tup_upd as updates,\n                        n_tup_del as deletes,\n                        n_live_tup as live_tuples,\n                        n_dead_tup as dead_tuples,\n                        seq_scan as sequential_scans,\n                        seq_tup_read as sequential_reads,\n                        idx_scan as index_scans,\n                        idx_tup_fetch as index_reads,\n                        n_tup_hot_upd as hot_updates,\n                        vacuum_count,\n                        autovacuum_count,\n                        analyze_count,\n                        autoanalyze_count,\n                        last_vacuum,\n                        last_autovacuum,\n                        last_analyze,\n                        last_autoanalyze\n                    FROM pg_stat_user_tables\n                    WHERE tablename = :table_name\n                \"\"\")\n                \n                table_result = await session.execute(table_stats_query, {\"table_name\": table_name})\n                table_stats = table_result.fetchone()\n                \n                if not table_stats:\n                    return {\"success\": False, \"error\": f\"Table {table_name} not found\"}\n                \n                # Table size\n                size_query = text(\"\"\"\n                    SELECT \n                        pg_size_pretty(pg_total_relation_size(:table_name)) as total_size,\n                        pg_total_relation_size(:table_name) as total_size_bytes,\n                        pg_size_pretty(pg_relation_size(:table_name)) as table_size,\n                        pg_relation_size(:table_name) as table_size_bytes\n                \"\"\")\n                \n                size_result = await session.execute(size_query, {\"table_name\": table_name})\n                size_stats = size_result.fetchone()\n                \n                # Index statistics for this table\n                index_stats_query = text(\"\"\"\n                    SELECT \n                        indexname,\n                        idx_scan as scans,\n                        idx_tup_read as tuples_read,\n                        idx_tup_fetch as tuples_fetched,\n                        pg_size_pretty(pg_relation_size(indexrelid)) as index_size\n                    FROM pg_stat_user_indexes\n                    WHERE tablename = :table_name\n                    ORDER BY idx_scan DESC\n                \"\"\")\n                \n                index_result = await session.execute(index_stats_query, {\"table_name\": table_name})\n                index_stats = []\n                \n                for row in index_result.fetchall():\n                    index_stats.append({\n                        \"index_name\": row.indexname,\n                        \"scans\": row.scans,\n                        \"tuples_read\": row.tuples_read,\n                        \"tuples_fetched\": row.tuples_fetched,\n                        \"index_size\": row.index_size\n                    })\n                \n                # Calculate performance metrics\n                dead_tuple_ratio = round(\n                    (table_stats.dead_tuples / (table_stats.live_tuples + 1)) * 100, 2\n                )\n                \n                index_vs_seq_ratio = round(\n                    (table_stats.index_scans / (table_stats.sequential_scans + 1)) * 100, 2\n                )\n                \n                # Performance recommendations\n                recommendations = []\n                \n                if dead_tuple_ratio > 10:\n                    recommendations.append(\"High dead tuple ratio - consider VACUUM\")\n                \n                if table_stats.sequential_scans > table_stats.index_scans:\n                    recommendations.append(\"More sequential scans than index scans - check query patterns\")\n                \n                if not table_stats.last_analyze or (datetime.utcnow() - table_stats.last_analyze).days > 7:\n                    recommendations.append(\"Table not analyzed recently - consider ANALYZE\")\n                \n                return {\n                    \"success\": True,\n                    \"table_name\": table_name,\n                    \"table_statistics\": {\n                        \"live_tuples\": table_stats.live_tuples,\n                        \"dead_tuples\": table_stats.dead_tuples,\n                        \"dead_tuple_ratio\": dead_tuple_ratio,\n                        \"inserts\": table_stats.inserts,\n                        \"updates\": table_stats.updates,\n                        \"deletes\": table_stats.deletes,\n                        \"sequential_scans\": table_stats.sequential_scans,\n                        \"sequential_reads\": table_stats.sequential_reads,\n                        \"index_scans\": table_stats.index_scans,\n                        \"index_reads\": table_stats.index_reads,\n                        \"hot_updates\": table_stats.hot_updates,\n                        \"index_vs_seq_ratio\": index_vs_seq_ratio\n                    },\n                    \"size_statistics\": {\n                        \"total_size\": size_stats.total_size,\n                        \"total_size_bytes\": size_stats.total_size_bytes,\n                        \"table_size\": size_stats.table_size,\n                        \"table_size_bytes\": size_stats.table_size_bytes\n                    },\n                    \"maintenance_info\": {\n                        \"vacuum_count\": table_stats.vacuum_count,\n                        \"autovacuum_count\": table_stats.autovacuum_count,\n                        \"analyze_count\": table_stats.analyze_count,\n                        \"autoanalyze_count\": table_stats.autoanalyze_count,\n                        \"last_vacuum\": table_stats.last_vacuum,\n                        \"last_autovacuum\": table_stats.last_autovacuum,\n                        \"last_analyze\": table_stats.last_analyze,\n                        \"last_autoanalyze\": table_stats.last_autoanalyze\n                    },\n                    \"indexes\": index_stats,\n                    \"recommendations\": recommendations,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to analyze table performance for {table_name}: {e}\")\n            return {\"success\": False, \"error\": str(e)}\n    \n    async def get_connection_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get database connection statistics.\"\"\"\n        try:\n            async with self.db_connection.get_async_session() as session:\n                # Connection statistics\n                conn_stats_query = text(\"\"\"\n                    SELECT \n                        state,\n                        count(*) as count,\n                        avg(extract(epoch from now() - state_change)) as avg_duration\n                    FROM pg_stat_activity\n                    WHERE datname = current_database()\n                    GROUP BY state\n                \"\"\")\n                \n                conn_result = await session.execute(conn_stats_query)\n                connection_stats = {}\n                \n                for row in conn_result.fetchall():\n                    connection_stats[row.state] = {\n                        \"count\": row.count,\n                        \"avg_duration_seconds\": round(row.avg_duration, 2) if row.avg_duration else 0\n                    }\n                \n                # Long-running queries\n                long_queries_query = text(\"\"\"\n                    SELECT \n                        pid,\n                        state,\n                        query,\n                        extract(epoch from now() - query_start) as duration_seconds\n                    FROM pg_stat_activity\n                    WHERE datname = current_database()\n                    AND state = 'active'\n                    AND now() - query_start > interval '30 seconds'\n                    ORDER BY duration_seconds DESC\n                \"\"\")\n                \n                long_result = await session.execute(long_queries_query)\n                long_queries = []\n                \n                for row in long_result.fetchall():\n                    long_queries.append({\n                        \"pid\": row.pid,\n                        \"state\": row.state,\n                        \"query\": row.query[:200] + \"...\" if len(row.query) > 200 else row.query,\n                        \"duration_seconds\": round(row.duration_seconds, 2)\n                    })\n                \n                return {\n                    \"success\": True,\n                    \"connection_states\": connection_stats,\n                    \"long_running_queries\": long_queries,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n        except Exception as e:\n            logger.error(f\"Failed to get connection statistics: {e}\")\n            return {\"success\": False, \"error\": str(e)}\n    \n    def set_slow_query_threshold(self, threshold_seconds: float):\n        \"\"\"Set slow query threshold.\"\"\"\n        self.slow_query_threshold = threshold_seconds\n        logger.info(f\"Slow query threshold set to {threshold_seconds}s\")\n    \n    def clear_metrics(self):\n        \"\"\"Clear stored metrics.\"\"\"\n        self.query_metrics.clear()\n        logger.info(\"Query metrics cleared\")\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of stored metrics.\"\"\"\n        if not self.query_metrics:\n            return {\"total_metrics\": 0, \"message\": \"No metrics available\"}\n        \n        total_metrics = len(self.query_metrics)\n        avg_execution_time = sum(m.execution_time for m in self.query_metrics) / total_metrics\n        slow_queries = sum(1 for m in self.query_metrics if m.execution_time > self.slow_query_threshold)\n        \n        return {\n            \"total_metrics\": total_metrics,\n            \"average_execution_time\": round(avg_execution_time, 3),\n            \"slow_queries_count\": slow_queries,\n            \"slow_queries_percentage\": round((slow_queries / total_metrics) * 100, 2),\n            \"threshold_seconds\": self.slow_query_threshold,\n            \"oldest_metric\": min(m.timestamp for m in self.query_metrics),\n            \"newest_metric\": max(m.timestamp for m in self.query_metrics)\n        }\n    \n    def _format_size(self, size_bytes: int) -> str:\n        \"\"\"Format size in human readable format.\"\"\"\n        for unit in ['B', 'KB', 'MB', 'GB']:\n            if size_bytes < 1024.0:\n                return f\"{size_bytes:.1f} {unit}\"\n            size_bytes /= 1024.0\n        return f\"{size_bytes:.1f} TB\""