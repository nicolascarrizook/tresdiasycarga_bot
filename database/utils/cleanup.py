"""
Database cleanup utilities for Sistema Mayra.
"""
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

from sqlalchemy import text, func, delete
from sqlalchemy.ext.asyncio import AsyncSession

logger = logging.getLogger(__name__)


class DatabaseCleanup:
    """Database cleanup manager for maintenance tasks."""
    
    def __init__(self, db_connection=None):
        self.db_connection = db_connection
        self.cleanup_history: List[Dict[str, Any]] = []
    
    async def cleanup_expired_cache(self, batch_size: int = 1000) -> Dict[str, Any]:
        """Clean up expired cache entries."""
        start_time = datetime.utcnow()
        
        try:
            if not self.db_connection:
                from database.utils.connection import get_database_connection
                self.db_connection = get_database_connection()
            
            from database.models.cache import CacheEntry
            
            async with self.db_connection.get_async_session() as session:
                # Find expired entries
                expired_query = text(\"\"\"\n                    SELECT id FROM cache_entries \n                    WHERE expires_at IS NOT NULL \n                    AND expires_at < NOW()\n                    LIMIT :batch_size\n                \"\"\")\n                \n                total_deleted = 0\n                \n                while True:\n                    result = await session.execute(expired_query, {\"batch_size\": batch_size})\n                    expired_ids = [row[0] for row in result.fetchall()]\n                    \n                    if not expired_ids:\n                        break\n                    \n                    # Delete expired entries\n                    delete_query = delete(CacheEntry).where(CacheEntry.id.in_(expired_ids))\n                    delete_result = await session.execute(delete_query)\n                    \n                    batch_deleted = delete_result.rowcount\n                    total_deleted += batch_deleted\n                    \n                    await session.commit()\n                    \n                    logger.info(f\"Deleted {batch_deleted} expired cache entries\")\n                    \n                    if batch_deleted < batch_size:\n                        break\n                \n                execution_time = (datetime.utcnow() - start_time).total_seconds()\n                \n                result = {\n                    \"task\": \"cleanup_expired_cache\",\n                    \"success\": True,\n                    \"deleted_count\": total_deleted,\n                    \"execution_time\": execution_time,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n                self.cleanup_history.append(result)\n                logger.info(f\"Cleaned up {total_deleted} expired cache entries in {execution_time:.2f}s\")\n                \n                return result\n                \n        except Exception as e:\n            execution_time = (datetime.utcnow() - start_time).total_seconds()\n            result = {\n                \"task\": \"cleanup_expired_cache\",\n                \"success\": False,\n                \"error\": str(e),\n                \"execution_time\": execution_time,\n                \"timestamp\": datetime.utcnow()\n            }\n            \n            self.cleanup_history.append(result)\n            logger.error(f\"Failed to cleanup expired cache: {e}\")\n            \n            return result\n    \n    async def cleanup_old_audit_logs(self, days: int = 90, batch_size: int = 1000) -> Dict[str, Any]:\n        \"\"\"Clean up old audit logs.\"\"\"\n        start_time = datetime.utcnow()\n        \n        try:\n            from database.models.audit import AuditLog, AuditActionEnum\n            \n            cutoff_date = datetime.utcnow() - timedelta(days=days)\n            \n            # Keep security-related logs longer\n            security_actions = [\n                AuditActionEnum.LOGIN,\n                AuditActionEnum.LOGOUT,\n                AuditActionEnum.SYSTEM_ERROR\n            ]\n            \n            async with self.db_connection.get_async_session() as session:\n                # Find old non-security logs\n                old_logs_query = text(\"\"\"\n                    SELECT id FROM audit_logs \n                    WHERE created_at < :cutoff_date\n                    AND action NOT IN :security_actions\n                    LIMIT :batch_size\n                \"\"\")\n                \n                total_deleted = 0\n                \n                while True:\n                    result = await session.execute(old_logs_query, {\n                        \"cutoff_date\": cutoff_date,\n                        \"security_actions\": tuple(security_actions),\n                        \"batch_size\": batch_size\n                    })\n                    old_log_ids = [row[0] for row in result.fetchall()]\n                    \n                    if not old_log_ids:\n                        break\n                    \n                    # Delete old logs\n                    delete_query = delete(AuditLog).where(AuditLog.id.in_(old_log_ids))\n                    delete_result = await session.execute(delete_query)\n                    \n                    batch_deleted = delete_result.rowcount\n                    total_deleted += batch_deleted\n                    \n                    await session.commit()\n                    \n                    logger.info(f\"Deleted {batch_deleted} old audit logs\")\n                    \n                    if batch_deleted < batch_size:\n                        break\n                \n                execution_time = (datetime.utcnow() - start_time).total_seconds()\n                \n                result = {\n                    \"task\": \"cleanup_old_audit_logs\",\n                    \"success\": True,\n                    \"deleted_count\": total_deleted,\n                    \"cutoff_date\": cutoff_date,\n                    \"execution_time\": execution_time,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n                self.cleanup_history.append(result)\n                logger.info(f\"Cleaned up {total_deleted} old audit logs in {execution_time:.2f}s\")\n                \n                return result\n                \n        except Exception as e:\n            execution_time = (datetime.utcnow() - start_time).total_seconds()\n            result = {\n                \"task\": \"cleanup_old_audit_logs\",\n                \"success\": False,\n                \"error\": str(e),\n                \"execution_time\": execution_time,\n                \"timestamp\": datetime.utcnow()\n            }\n            \n            self.cleanup_history.append(result)\n            logger.error(f\"Failed to cleanup old audit logs: {e}\")\n            \n            return result\n    \n    async def cleanup_unused_embeddings(self, days: int = 30, batch_size: int = 1000) -> Dict[str, Any]:\n        \"\"\"Clean up unused embeddings.\"\"\"\n        start_time = datetime.utcnow()\n        \n        try:\n            from database.models.embedding import Embedding\n            \n            cutoff_date = datetime.utcnow() - timedelta(days=days)\n            \n            async with self.db_connection.get_async_session() as session:\n                # Find unused embeddings\n                unused_query = text(\"\"\"\n                    SELECT id FROM embeddings \n                    WHERE (last_retrieved_at IS NULL OR last_retrieved_at < :cutoff_date)\n                    AND retrieval_count = 0\n                    LIMIT :batch_size\n                \"\"\")\n                \n                total_deleted = 0\n                \n                while True:\n                    result = await session.execute(unused_query, {\n                        \"cutoff_date\": cutoff_date,\n                        \"batch_size\": batch_size\n                    })\n                    unused_ids = [row[0] for row in result.fetchall()]\n                    \n                    if not unused_ids:\n                        break\n                    \n                    # Delete unused embeddings\n                    delete_query = delete(Embedding).where(Embedding.id.in_(unused_ids))\n                    delete_result = await session.execute(delete_query)\n                    \n                    batch_deleted = delete_result.rowcount\n                    total_deleted += batch_deleted\n                    \n                    await session.commit()\n                    \n                    logger.info(f\"Deleted {batch_deleted} unused embeddings\")\n                    \n                    if batch_deleted < batch_size:\n                        break\n                \n                execution_time = (datetime.utcnow() - start_time).total_seconds()\n                \n                result = {\n                    \"task\": \"cleanup_unused_embeddings\",\n                    \"success\": True,\n                    \"deleted_count\": total_deleted,\n                    \"cutoff_date\": cutoff_date,\n                    \"execution_time\": execution_time,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n                self.cleanup_history.append(result)\n                logger.info(f\"Cleaned up {total_deleted} unused embeddings in {execution_time:.2f}s\")\n                \n                return result\n                \n        except Exception as e:\n            execution_time = (datetime.utcnow() - start_time).total_seconds()\n            result = {\n                \"task\": \"cleanup_unused_embeddings\",\n                \"success\": False,\n                \"error\": str(e),\n                \"execution_time\": execution_time,\n                \"timestamp\": datetime.utcnow()\n            }\n            \n            self.cleanup_history.append(result)\n            logger.error(f\"Failed to cleanup unused embeddings: {e}\")\n            \n            return result\n    \n    async def cleanup_stale_conversations(self, hours: int = 24, batch_size: int = 1000) -> Dict[str, Any]:\n        \"\"\"Clean up stale conversations.\"\"\"\n        start_time = datetime.utcnow()\n        \n        try:\n            from database.models.conversation import Conversation, ConversationStatusEnum\n            \n            cutoff_date = datetime.utcnow() - timedelta(hours=hours)\n            \n            async with self.db_connection.get_async_session() as session:\n                # Find stale active conversations\n                stale_query = text(\"\"\"\n                    SELECT id FROM conversations \n                    WHERE status = :active_status\n                    AND (last_message_at IS NULL OR last_message_at < :cutoff_date)\n                    LIMIT :batch_size\n                \"\"\")\n                \n                total_updated = 0\n                \n                while True:\n                    result = await session.execute(stale_query, {\n                        \"active_status\": ConversationStatusEnum.ACTIVE,\n                        \"cutoff_date\": cutoff_date,\n                        \"batch_size\": batch_size\n                    })\n                    stale_ids = [row[0] for row in result.fetchall()]\n                    \n                    if not stale_ids:\n                        break\n                    \n                    # Update stale conversations to cancelled\n                    update_query = text(\"\"\"\n                        UPDATE conversations \n                        SET status = :cancelled_status,\n                            completed_at = NOW(),\n                            error_message = 'Automatically cancelled due to inactivity'\n                        WHERE id IN :stale_ids\n                    \"\"\")\n                    \n                    update_result = await session.execute(update_query, {\n                        \"cancelled_status\": ConversationStatusEnum.CANCELLED,\n                        \"stale_ids\": tuple(stale_ids)\n                    })\n                    \n                    batch_updated = update_result.rowcount\n                    total_updated += batch_updated\n                    \n                    await session.commit()\n                    \n                    logger.info(f\"Updated {batch_updated} stale conversations\")\n                    \n                    if batch_updated < batch_size:\n                        break\n                \n                execution_time = (datetime.utcnow() - start_time).total_seconds()\n                \n                result = {\n                    \"task\": \"cleanup_stale_conversations\",\n                    \"success\": True,\n                    \"updated_count\": total_updated,\n                    \"cutoff_date\": cutoff_date,\n                    \"execution_time\": execution_time,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n                self.cleanup_history.append(result)\n                logger.info(f\"Cleaned up {total_updated} stale conversations in {execution_time:.2f}s\")\n                \n                return result\n                \n        except Exception as e:\n            execution_time = (datetime.utcnow() - start_time).total_seconds()\n            result = {\n                \"task\": \"cleanup_stale_conversations\",\n                \"success\": False,\n                \"error\": str(e),\n                \"execution_time\": execution_time,\n                \"timestamp\": datetime.utcnow()\n            }\n            \n            self.cleanup_history.append(result)\n            logger.error(f\"Failed to cleanup stale conversations: {e}\")\n            \n            return result\n    \n    async def cleanup_expired_plans(self) -> Dict[str, Any]:\n        \"\"\"Clean up expired plans.\"\"\"\n        start_time = datetime.utcnow()\n        \n        try:\n            from database.models.plan import Plan, PlanStatusEnum\n            \n            today = datetime.utcnow().date()\n            \n            async with self.db_connection.get_async_session() as session:\n                # Find expired active plans\n                expired_query = text(\"\"\"\n                    SELECT id FROM plans \n                    WHERE status = :active_status\n                    AND end_date IS NOT NULL\n                    AND end_date < :today\n                \"\"\")\n                \n                result = await session.execute(expired_query, {\n                    \"active_status\": PlanStatusEnum.ACTIVE,\n                    \"today\": today\n                })\n                expired_ids = [row[0] for row in result.fetchall()]\n                \n                if not expired_ids:\n                    execution_time = (datetime.utcnow() - start_time).total_seconds()\n                    result = {\n                        \"task\": \"cleanup_expired_plans\",\n                        \"success\": True,\n                        \"updated_count\": 0,\n                        \"execution_time\": execution_time,\n                        \"timestamp\": datetime.utcnow()\n                    }\n                    \n                    self.cleanup_history.append(result)\n                    return result\n                \n                # Update expired plans to completed\n                update_query = text(\"\"\"\n                    UPDATE plans \n                    SET status = :completed_status,\n                        is_active = false,\n                        deactivated_at = NOW()\n                    WHERE id IN :expired_ids\n                \"\"\")\n                \n                update_result = await session.execute(update_query, {\n                    \"completed_status\": PlanStatusEnum.COMPLETED,\n                    \"expired_ids\": tuple(expired_ids)\n                })\n                \n                updated_count = update_result.rowcount\n                \n                await session.commit()\n                \n                execution_time = (datetime.utcnow() - start_time).total_seconds()\n                \n                result = {\n                    \"task\": \"cleanup_expired_plans\",\n                    \"success\": True,\n                    \"updated_count\": updated_count,\n                    \"execution_time\": execution_time,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n                self.cleanup_history.append(result)\n                logger.info(f\"Cleaned up {updated_count} expired plans in {execution_time:.2f}s\")\n                \n                return result\n                \n        except Exception as e:\n            execution_time = (datetime.utcnow() - start_time).total_seconds()\n            result = {\n                \"task\": \"cleanup_expired_plans\",\n                \"success\": False,\n                \"error\": str(e),\n                \"execution_time\": execution_time,\n                \"timestamp\": datetime.utcnow()\n            }\n            \n            self.cleanup_history.append(result)\n            logger.error(f\"Failed to cleanup expired plans: {e}\")\n            \n            return result\n    \n    async def vacuum_analyze_tables(self, table_names: Optional[List[str]] = None) -> Dict[str, Any]:\n        \"\"\"Vacuum and analyze tables.\"\"\"\n        start_time = datetime.utcnow()\n        \n        try:\n            async with self.db_connection.get_async_session() as session:\n                if table_names is None:\n                    # Get all user tables\n                    tables_result = await session.execute(text(\"\"\"\n                        SELECT tablename FROM pg_tables \n                        WHERE schemaname = 'public'\n                    \"\"\"))\n                    table_names = [row[0] for row in tables_result.fetchall()]\n                \n                vacuum_results = []\n                \n                for table_name in table_names:\n                    try:\n                        # Vacuum and analyze table\n                        await session.execute(text(f\"VACUUM ANALYZE {table_name}\"))\n                        await session.commit()\n                        \n                        vacuum_results.append({\n                            \"table\": table_name,\n                            \"success\": True\n                        })\n                        \n                        logger.info(f\"Vacuumed and analyzed table: {table_name}\")\n                        \n                    except Exception as e:\n                        vacuum_results.append({\n                            \"table\": table_name,\n                            \"success\": False,\n                            \"error\": str(e)\n                        })\n                        \n                        logger.error(f\"Failed to vacuum table {table_name}: {e}\")\n                \n                execution_time = (datetime.utcnow() - start_time).total_seconds()\n                \n                successful_count = sum(1 for r in vacuum_results if r[\"success\"])\n                \n                result = {\n                    \"task\": \"vacuum_analyze_tables\",\n                    \"success\": True,\n                    \"processed_tables\": len(table_names),\n                    \"successful_count\": successful_count,\n                    \"failed_count\": len(table_names) - successful_count,\n                    \"table_results\": vacuum_results,\n                    \"execution_time\": execution_time,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n                self.cleanup_history.append(result)\n                logger.info(f\"Vacuumed {successful_count}/{len(table_names)} tables in {execution_time:.2f}s\")\n                \n                return result\n                \n        except Exception as e:\n            execution_time = (datetime.utcnow() - start_time).total_seconds()\n            result = {\n                \"task\": \"vacuum_analyze_tables\",\n                \"success\": False,\n                \"error\": str(e),\n                \"execution_time\": execution_time,\n                \"timestamp\": datetime.utcnow()\n            }\n            \n            self.cleanup_history.append(result)\n            logger.error(f\"Failed to vacuum tables: {e}\")\n            \n            return result\n    \n    async def cleanup_soft_deleted_records(self, days: int = 30, batch_size: int = 1000) -> Dict[str, Any]:\n        \"\"\"Clean up soft deleted records.\"\"\"\n        start_time = datetime.utcnow()\n        \n        try:\n            cutoff_date = datetime.utcnow() - timedelta(days=days)\n            \n            # Tables with soft delete\n            soft_delete_tables = [\n                \"users\", \"patients\", \"recipes\", \"recipe_ingredients\", \n                \"plans\", \"conversations\", \"embeddings\", \"audit_logs\", \"cache_entries\"\n            ]\n            \n            total_deleted = 0\n            table_results = []\n            \n            async with self.db_connection.get_async_session() as session:\n                for table_name in soft_delete_tables:\n                    try:\n                        # Find soft deleted records older than cutoff\n                        find_query = text(f\"\"\"\n                            SELECT id FROM {table_name} \n                            WHERE is_deleted = true \n                            AND deleted_at < :cutoff_date\n                            LIMIT :batch_size\n                        \"\"\")\n                        \n                        table_deleted = 0\n                        \n                        while True:\n                            result = await session.execute(find_query, {\n                                \"cutoff_date\": cutoff_date,\n                                \"batch_size\": batch_size\n                            })\n                            soft_deleted_ids = [row[0] for row in result.fetchall()]\n                            \n                            if not soft_deleted_ids:\n                                break\n                            \n                            # Permanently delete records\n                            delete_query = text(f\"\"\"\n                                DELETE FROM {table_name} \n                                WHERE id IN :ids\n                            \"\"\")\n                            \n                            delete_result = await session.execute(delete_query, {\n                                \"ids\": tuple(soft_deleted_ids)\n                            })\n                            \n                            batch_deleted = delete_result.rowcount\n                            table_deleted += batch_deleted\n                            \n                            await session.commit()\n                            \n                            if batch_deleted < batch_size:\n                                break\n                        \n                        table_results.append({\n                            \"table\": table_name,\n                            \"deleted_count\": table_deleted,\n                            \"success\": True\n                        })\n                        \n                        total_deleted += table_deleted\n                        \n                        if table_deleted > 0:\n                            logger.info(f\"Deleted {table_deleted} soft deleted records from {table_name}\")\n                        \n                    except Exception as e:\n                        table_results.append({\n                            \"table\": table_name,\n                            \"deleted_count\": 0,\n                            \"success\": False,\n                            \"error\": str(e)\n                        })\n                        \n                        logger.error(f\"Failed to cleanup soft deleted records from {table_name}: {e}\")\n                \n                execution_time = (datetime.utcnow() - start_time).total_seconds()\n                \n                result = {\n                    \"task\": \"cleanup_soft_deleted_records\",\n                    \"success\": True,\n                    \"total_deleted\": total_deleted,\n                    \"cutoff_date\": cutoff_date,\n                    \"table_results\": table_results,\n                    \"execution_time\": execution_time,\n                    \"timestamp\": datetime.utcnow()\n                }\n                \n                self.cleanup_history.append(result)\n                logger.info(f\"Cleaned up {total_deleted} soft deleted records in {execution_time:.2f}s\")\n                \n                return result\n                \n        except Exception as e:\n            execution_time = (datetime.utcnow() - start_time).total_seconds()\n            result = {\n                \"task\": \"cleanup_soft_deleted_records\",\n                \"success\": False,\n                \"error\": str(e),\n                \"execution_time\": execution_time,\n                \"timestamp\": datetime.utcnow()\n            }\n            \n            self.cleanup_history.append(result)\n            logger.error(f\"Failed to cleanup soft deleted records: {e}\")\n            \n            return result\n    \n    async def run_full_cleanup(self) -> Dict[str, Any]:\n        \"\"\"Run all cleanup tasks.\"\"\"\n        start_time = datetime.utcnow()\n        \n        cleanup_tasks = [\n            self.cleanup_expired_cache(),\n            self.cleanup_old_audit_logs(),\n            self.cleanup_unused_embeddings(),\n            self.cleanup_stale_conversations(),\n            self.cleanup_expired_plans(),\n            self.cleanup_soft_deleted_records(),\n            self.vacuum_analyze_tables()\n        ]\n        \n        results = []\n        \n        for task in cleanup_tasks:\n            try:\n                result = await task\n                results.append(result)\n            except Exception as e:\n                logger.error(f\"Cleanup task failed: {e}\")\n                results.append({\n                    \"task\": \"unknown\",\n                    \"success\": False,\n                    \"error\": str(e),\n                    \"timestamp\": datetime.utcnow()\n                })\n        \n        execution_time = (datetime.utcnow() - start_time).total_seconds()\n        \n        successful_tasks = sum(1 for r in results if r[\"success\"])\n        \n        full_cleanup_result = {\n            \"task\": \"full_cleanup\",\n            \"success\": successful_tasks == len(results),\n            \"total_tasks\": len(results),\n            \"successful_tasks\": successful_tasks,\n            \"failed_tasks\": len(results) - successful_tasks,\n            \"task_results\": results,\n            \"execution_time\": execution_time,\n            \"timestamp\": datetime.utcnow()\n        }\n        \n        self.cleanup_history.append(full_cleanup_result)\n        logger.info(f\"Full cleanup completed: {successful_tasks}/{len(results)} tasks successful in {execution_time:.2f}s\")\n        \n        return full_cleanup_result\n    \n    def get_cleanup_history(self, hours: int = 24) -> List[Dict[str, Any]]:\n        \"\"\"Get cleanup history.\"\"\"\n        cutoff_time = datetime.utcnow() - timedelta(hours=hours)\n        \n        return [\n            result for result in self.cleanup_history\n            if result[\"timestamp\"] >= cutoff_time\n        ]\n    \n    def get_cleanup_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get cleanup statistics.\"\"\"\n        if not self.cleanup_history:\n            return {\"status\": \"no_data\"}\n        \n        recent_history = self.get_cleanup_history(24)\n        \n        if not recent_history:\n            return {\"status\": \"no_recent_data\"}\n        \n        total_tasks = len(recent_history)\n        successful_tasks = sum(1 for r in recent_history if r[\"success\"])\n        \n        # Calculate total execution time\n        total_execution_time = sum(r.get(\"execution_time\", 0) for r in recent_history)\n        \n        # Count by task type\n        task_counts = {}\n        for result in recent_history:\n            task = result[\"task\"]\n            if task not in task_counts:\n                task_counts[task] = {\"total\": 0, \"successful\": 0}\n            task_counts[task][\"total\"] += 1\n            if result[\"success\"]:\n                task_counts[task][\"successful\"] += 1\n        \n        return {\n            \"total_tasks\": total_tasks,\n            \"successful_tasks\": successful_tasks,\n            \"failed_tasks\": total_tasks - successful_tasks,\n            \"success_rate\": (successful_tasks / total_tasks) * 100 if total_tasks > 0 else 0,\n            \"total_execution_time\": total_execution_time,\n            \"average_execution_time\": total_execution_time / total_tasks if total_tasks > 0 else 0,\n            \"task_breakdown\": task_counts,\n            \"last_cleanup\": max(r[\"timestamp\"] for r in recent_history) if recent_history else None\n        }"